{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting query1.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query1.hql\n",
    "\n",
    "USE demodb;\n",
    "\n",
    "DROP TABLE if exists posts_sample_external; \n",
    "\n",
    "CREATE EXTERNAL TABLE posts_sample_external \n",
    "(year string,\n",
    "month string)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' \n",
    "WITH SERDEPROPERTIES (\n",
    "    \"input.regex\" = '.*?(?=.*\\\\bCreationDate=\\\"(\\\\d+)-(\\\\d+)).*'\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/data/stackexchange1000/posts/';\n",
    "\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.dynamic.partition=true;\n",
    "set hive.exec.max.dynamic.partitions=2000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=1000;\n",
    "\n",
    "\n",
    "DROP TABLE if exists posts_sample; \n",
    "\n",
    "CREATE TABLE posts_sample \n",
    "(count int) \n",
    "PARTITIONED BY (year string, month string)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t';\n",
    "\n",
    "FROM posts_sample_external\n",
    "INSERT OVERWRITE TABLE posts_sample\n",
    "PARTITION (year, month)\n",
    "SELECT count(*) as count, year, concat(year,\"-\",month) as month\n",
    "GROUP BY year, month;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting query2.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query2.hql\n",
    "\n",
    "USE demodb;\n",
    "\n",
    "SELECT year, month, count FROM (\n",
    "    SELECT * FROM (\n",
    "        SELECT\n",
    "            count,\n",
    "            year,\n",
    "            month,\n",
    "            ROW_NUMBER() OVER(ORDER BY year, month ASC) as rowid\n",
    "        FROM posts_sample\n",
    "    ) AS q1\n",
    "    ORDER BY rowid ASC\n",
    ") AS q2\n",
    "WHERE rowid='3';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hive -S -f query1.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.155 seconds\n",
      "Query ID = jovyan_20180716045757_6d137af4-c5d7-446f-a4a9-9a13d708b00c\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1531693512718_0040, Tracking URL = http://b5a379c75491:8088/proxy/application_1531693512718_0040/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1531693512718_0040\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2018-07-16 04:57:39,761 Stage-1 map = 0%,  reduce = 0%\n",
      "2018-07-16 04:57:48,599 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.24 sec\n",
      "2018-07-16 04:57:58,458 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.7 sec\n",
      "MapReduce Total cumulative CPU time: 11 seconds 700 msec\n",
      "Ended Job = job_1531693512718_0040\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1531693512718_0041, Tracking URL = http://b5a379c75491:8088/proxy/application_1531693512718_0041/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1531693512718_0041\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2018-07-16 04:58:14,802 Stage-2 map = 0%,  reduce = 0%\n",
      "2018-07-16 04:58:22,389 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.41 sec\n",
      "2018-07-16 04:58:32,046 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.57 sec\n",
      "MapReduce Total cumulative CPU time: 5 seconds 570 msec\n",
      "Ended Job = job_1531693512718_0041\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 11.7 sec   HDFS Read: 48268 HDFS Write: 128 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 5.57 sec   HDFS Read: 4838 HDFS Write: 16 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 17 seconds 270 msec\n",
      "OK\n",
      "2008\t2008-10\t73\n",
      "Time taken: 71.351 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -f query2.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
